{"ast":null,"code":"\"use strict\";\n\nvar _regeneratorRuntime = require(\"C:\\\\Users\\\\RajaJ\\\\OneDrive\\\\Desktop\\\\MedNet_App\\\\node_modules\\\\babel-preset-react-app\\\\node_modules\\\\@babel\\\\runtime/regenerator\");\n\nvar _classCallCheck = require(\"C:\\\\Users\\\\RajaJ\\\\OneDrive\\\\Desktop\\\\MedNet_App\\\\node_modules\\\\babel-preset-react-app\\\\node_modules\\\\@babel\\\\runtime/helpers/classCallCheck\");\n\nvar _createClass = require(\"C:\\\\Users\\\\RajaJ\\\\OneDrive\\\\Desktop\\\\MedNet_App\\\\node_modules\\\\babel-preset-react-app\\\\node_modules\\\\@babel\\\\runtime/helpers/createClass\");\n\nvar __awaiter = this && this.__awaiter || function (thisArg, _arguments, P, generator) {\n  function adopt(value) {\n    return value instanceof P ? value : new P(function (resolve) {\n      resolve(value);\n    });\n  }\n\n  return new (P || (P = Promise))(function (resolve, reject) {\n    function fulfilled(value) {\n      try {\n        step(generator.next(value));\n      } catch (e) {\n        reject(e);\n      }\n    }\n\n    function rejected(value) {\n      try {\n        step(generator[\"throw\"](value));\n      } catch (e) {\n        reject(e);\n      }\n    }\n\n    function step(result) {\n      result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected);\n    }\n\n    step((generator = generator.apply(thisArg, _arguments || [])).next());\n  });\n};\n\nvar __importDefault = this && this.__importDefault || function (mod) {\n  return mod && mod.__esModule ? mod : {\n    \"default\": mod\n  };\n};\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.BrowserMicrophone = void 0;\n\nvar types_1 = require(\"./types\");\n\nvar audioworklet_1 = __importDefault(require(\"./audioworklet\"));\n\nvar audioProcessEvent = 'audioprocess';\nvar baseBufferSize = 4096;\n\nvar BrowserMicrophone = /*#__PURE__*/function () {\n  function BrowserMicrophone(isWebkit, sampleRate, apiClient) {\n    var _this = this;\n\n    var debug = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : false;\n\n    _classCallCheck(this, BrowserMicrophone);\n\n    this.initialized = false;\n    this.muted = false;\n    this.stats = {\n      maxSignalEnergy: 0.0\n    };\n\n    this.handleAudio = function (array) {\n      if (_this.muted) {\n        return;\n      }\n\n      if (array.length > 0) {\n        _this.apiClient.sendAudio(array);\n      }\n    };\n\n    this.isWebkit = isWebkit;\n    this.apiClient = apiClient;\n    this.sampleRate = sampleRate;\n    this.debug = debug;\n  }\n\n  _createClass(BrowserMicrophone, [{\n    key: \"initialize\",\n    value: function initialize(audioContext, mediaStreamConstraints) {\n      var _a;\n\n      return __awaiter(this, void 0, void 0, /*#__PURE__*/_regeneratorRuntime.mark(function _callee() {\n        var _this2 = this;\n\n        var blob, blobURL, speechlyNode, controlSAB, dataSAB, bufSize;\n        return _regeneratorRuntime.wrap(function _callee$(_context) {\n          while (1) {\n            switch (_context.prev = _context.next) {\n              case 0:\n                if (!(((_a = window.navigator) === null || _a === void 0 ? void 0 : _a.mediaDevices) === undefined)) {\n                  _context.next = 2;\n                  break;\n                }\n\n                throw types_1.ErrDeviceNotSupported;\n\n              case 2:\n                this.audioContext = audioContext;\n                this.resampleRatio = this.audioContext.sampleRate / this.sampleRate;\n                _context.prev = 4;\n                _context.next = 7;\n                return window.navigator.mediaDevices.getUserMedia(mediaStreamConstraints);\n\n              case 7:\n                this.mediaStream = _context.sent;\n                _context.next = 13;\n                break;\n\n              case 10:\n                _context.prev = 10;\n                _context.t0 = _context[\"catch\"](4);\n                throw types_1.ErrNoAudioConsent;\n\n              case 13:\n                this.audioTrack = this.mediaStream.getAudioTracks()[0]; // Start audio context if we are dealing with a non-WebKit browser.\n                //\n                // Non-webkit browsers (currently only Chrome on Android)\n                // require that user media is obtained before resuming the audio context.\n                //\n                // If audio context is attempted to be resumed before `mediaDevices.getUserMedia`,\n                // `audioContext.resume()` will hang indefinitely, without being resolved or rejected.\n\n                if (this.isWebkit) {\n                  _context.next = 17;\n                  break;\n                }\n\n                _context.next = 17;\n                return this.audioContext.resume();\n\n              case 17:\n                if (!(window.AudioWorkletNode !== undefined)) {\n                  _context.next = 29;\n                  break;\n                }\n\n                blob = new Blob([audioworklet_1.default], {\n                  type: 'text/javascript'\n                });\n                blobURL = window.URL.createObjectURL(blob);\n                _context.next = 22;\n                return this.audioContext.audioWorklet.addModule(blobURL);\n\n              case 22:\n                speechlyNode = new AudioWorkletNode(this.audioContext, 'speechly-worklet');\n                this.audioContext.createMediaStreamSource(this.mediaStream).connect(speechlyNode);\n                speechlyNode.connect(this.audioContext.destination); // @ts-ignore\n\n                if (window.SharedArrayBuffer !== undefined) {\n                  // Chrome, Edge, Firefox, Firefox Android\n                  // @ts-ignore\n                  controlSAB = new window.SharedArrayBuffer(4 * Int32Array.BYTES_PER_ELEMENT); // @ts-ignore\n\n                  dataSAB = new window.SharedArrayBuffer(1024 * Float32Array.BYTES_PER_ELEMENT);\n                  this.apiClient.postMessage({\n                    type: 'SET_SHARED_ARRAY_BUFFERS',\n                    controlSAB: controlSAB,\n                    dataSAB: dataSAB\n                  });\n                  speechlyNode.port.postMessage({\n                    type: 'SET_SHARED_ARRAY_BUFFERS',\n                    controlSAB: controlSAB,\n                    dataSAB: dataSAB,\n                    debug: this.debug\n                  });\n                } else {\n                  // Opera, Chrome Android, Webview Anroid\n                  if (this.debug) {\n                    console.log('[SpeechlyClient]', 'can not use SharedArrayBuffer');\n                  }\n                }\n\n                speechlyNode.port.onmessage = function (event) {\n                  switch (event.data.type) {\n                    case 'STATS':\n                      if (event.data.signalEnergy > _this2.stats.maxSignalEnergy) {\n                        _this2.stats.maxSignalEnergy = event.data.signalEnergy;\n                      }\n\n                      break;\n\n                    case 'DATA':\n                      _this2.handleAudio(event.data.frames);\n\n                      break;\n\n                    default:\n                  }\n                };\n\n                _context.next = 34;\n                break;\n\n              case 29:\n                if (this.debug) {\n                  console.log('[SpeechlyClient]', 'can not use AudioWorkletNode');\n                } // Safari, iOS Safari and Internet Explorer\n\n\n                if (this.isWebkit) {\n                  // Multiply base buffer size of 4 kB by the resample ratio rounded up to the next power of 2.\n                  // i.e. for 48 kHz to 16 kHz downsampling, this will be 4096 (base) * 4 = 16384.\n                  bufSize = baseBufferSize * Math.pow(2, Math.ceil(Math.log(this.resampleRatio) / Math.log(2)));\n                  this.audioProcessor = this.audioContext.createScriptProcessor(bufSize, 1, 1);\n                } else {\n                  this.audioProcessor = this.audioContext.createScriptProcessor(undefined, 1, 1);\n                }\n\n                this.audioContext.createMediaStreamSource(this.mediaStream).connect(this.audioProcessor);\n                this.audioProcessor.connect(this.audioContext.destination);\n                this.audioProcessor.addEventListener(audioProcessEvent, function (event) {\n                  _this2.handleAudio(event.inputBuffer.getChannelData(0));\n                });\n\n              case 34:\n                this.initialized = true;\n                this.mute();\n\n              case 36:\n              case \"end\":\n                return _context.stop();\n            }\n          }\n        }, _callee, this, [[4, 10]]);\n      }));\n    }\n  }, {\n    key: \"close\",\n    value: function close() {\n      return __awaiter(this, void 0, void 0, /*#__PURE__*/_regeneratorRuntime.mark(function _callee2() {\n        var t, stream, proc;\n        return _regeneratorRuntime.wrap(function _callee2$(_context2) {\n          while (1) {\n            switch (_context2.prev = _context2.next) {\n              case 0:\n                this.mute();\n\n                if (this.initialized) {\n                  _context2.next = 3;\n                  break;\n                }\n\n                throw types_1.ErrNotInitialized;\n\n              case 3:\n                t = this.audioTrack;\n                t.enabled = false; // Stop all media tracks\n\n                stream = this.mediaStream;\n                stream.getTracks().forEach(function (t) {\n                  return t.stop();\n                }); // Disconnect and stop ScriptProcessorNode\n\n                if (this.audioProcessor != null) {\n                  proc = this.audioProcessor;\n                  proc.disconnect();\n                } // Unset all audio infrastructure\n\n\n                this.mediaStream = undefined;\n                this.audioTrack = undefined;\n                this.audioProcessor = undefined;\n                this.initialized = false;\n\n              case 12:\n              case \"end\":\n                return _context2.stop();\n            }\n          }\n        }, _callee2, this);\n      }));\n    }\n  }, {\n    key: \"mute\",\n    value: function mute() {\n      this.muted = true;\n    }\n  }, {\n    key: \"unmute\",\n    value: function unmute() {\n      this.muted = false;\n    }\n    /**\n     * print statistics to console\n     */\n\n  }, {\n    key: \"printStats\",\n    value: function printStats() {\n      if (this.audioTrack != null) {\n        var settings = this.audioTrack.getSettings();\n        console.log(this.audioTrack.label, this.audioTrack.readyState); // @ts-ignore\n\n        console.log('channelCount', settings.channelCount); // @ts-ignore\n\n        console.log('latency', settings.latency); // @ts-ignore\n\n        console.log('autoGainControl', settings.autoGainControl);\n      }\n\n      console.log('maxSignalEnergy', this.stats.maxSignalEnergy);\n    }\n  }]);\n\n  return BrowserMicrophone;\n}();\n\nexports.BrowserMicrophone = BrowserMicrophone;","map":null,"metadata":{},"sourceType":"script"}